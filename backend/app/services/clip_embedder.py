"""
CLIP Embedding æœåŠ¡

ä½¿ç”¨ OpenAI CLIP æ¨¡å‹æå–å›¾åƒç‰¹å¾
- æ¨¡å‹ï¼šopenai/clip-vit-base-patch32 (å¯ç¦»çº¿è¿è¡Œ)
- ç‰¹å¾ç»´åº¦ï¼š512
- ä¼˜ç‚¹ï¼šé›¶æ ·æœ¬è¯†åˆ«èƒ½åŠ›å¼ºã€é€šç”¨æ€§å¥½
"""

import os

# è®¾ç½® HuggingFace é•œåƒæºï¼ˆå¿…é¡»åœ¨å¯¼å…¥ transformers ä¹‹å‰ï¼‰
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
# ç¦ç”¨ SSL éªŒè¯ï¼ˆè§£å†³è¯ä¹¦é—®é¢˜ï¼‰
os.environ["CURL_CA_BUNDLE"] = ""
os.environ["REQUESTS_CA_BUNDLE"] = ""

import torch
import numpy as np
from PIL import Image
from transformers import CLIPProcessor, CLIPModel
from typing import Union, List


class CLIPEmbedder:
    """
    CLIP å›¾åƒç‰¹å¾æå–å™¨
    
    å¯é€‰æ¨¡å‹æ–¹æ¡ˆï¼š
    1. openai/clip-vit-base-patch32 (æ¨è)
       - ç‰¹å¾ç»´åº¦ï¼š512
       - æ¨¡å‹å¤§å°ï¼š~350MB
       - æ¨ç†é€Ÿåº¦ï¼šå¿«
       - ç²¾åº¦ï¼šé«˜
    
    2. openai/clip-vit-large-patch14 (é«˜ç²¾åº¦)
       - ç‰¹å¾ç»´åº¦ï¼š768
       - æ¨¡å‹å¤§å°ï¼š~890MB
       - æ¨ç†é€Ÿåº¦ï¼šè¾ƒæ…¢
       - ç²¾åº¦ï¼šæ›´é«˜
    
    3. laion/CLIP-ViT-B-32-laion2B-s34B-b79K (å¤§æ•°æ®é›†è®­ç»ƒ)
       - ç‰¹å¾ç»´åº¦ï¼š512
       - æ¨¡å‹å¤§å°ï¼š~350MB
       - é€‚åˆé€šç”¨åœºæ™¯
    """
    
    def __init__(
        self,
        model_name: str = "openai/clip-vit-base-patch32",
        device: str = None,
        cache_dir: str = "./models"
    ):
        """
        åˆå§‹åŒ– CLIP æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            device: è®¾å¤‡ (cuda/cpu)ï¼ŒNone æ—¶è‡ªåŠ¨æ£€æµ‹
            cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•
        """
        self.model_name = model_name
        self.cache_dir = cache_dir
        
        # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
        
        print(f"ğŸ”§ åˆå§‹åŒ– CLIP æ¨¡å‹: {model_name}")
        print(f"ğŸ“ è®¾å¤‡: {self.device}")
        print(f"ğŸ’¾ ç¼“å­˜ç›®å½•: {cache_dir}")
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        os.makedirs(cache_dir, exist_ok=True)
        
        # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨ï¼ˆä½¿ç”¨å¿«é€Ÿç‰ˆæœ¬ï¼‰
        self.processor = CLIPProcessor.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            use_fast=True
        )
        
        self.model = CLIPModel.from_pretrained(
            model_name,
            cache_dir=cache_dir
        ).to(self.device)
        
        self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        
        # å¦‚æœæœ‰ GPUï¼Œä½¿ç”¨åŠç²¾åº¦åŠ é€Ÿ
        if self.device == "cuda":
            self.model = self.model.half()
            print("âš¡ å·²å¯ç”¨ FP16 åŠç²¾åº¦åŠ é€Ÿ")
        
        # è·å–ç‰¹å¾ç»´åº¦
        self.embedding_dim = self.model.config.projection_dim
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œç‰¹å¾ç»´åº¦: {self.embedding_dim}")
    
    def extract_image_features(
        self,
        image: Union[str, Image.Image],
        normalize: bool = True
    ) -> np.ndarray:
        """
        æå–å•å¼ å›¾ç‰‡çš„ç‰¹å¾å‘é‡
        
        Args:
            image: å›¾ç‰‡è·¯å¾„æˆ– PIL Image å¯¹è±¡
            normalize: æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾å‘é‡
            
        Returns:
            ç‰¹å¾å‘é‡ (1D numpy array)
        """
        # åŠ è½½å›¾ç‰‡
        if isinstance(image, str):
            image = Image.open(image).convert("RGB")
        elif not isinstance(image, Image.Image):
            raise ValueError("image must be a file path or PIL Image")
        
        # é¢„å¤„ç†
        inputs = self.processor(
            images=image,
            return_tensors="pt",
            padding=True
        ).to(self.device)
        
        # æå–ç‰¹å¾
        with torch.no_grad():
            image_features = self.model.get_image_features(**inputs)
        
        # è½¬æ¢ä¸º numpy
        features = image_features.cpu().numpy().flatten()
        
        # å½’ä¸€åŒ–
        if normalize:
            features = features / np.linalg.norm(features)
        
        return features.astype(np.float32)
    
    def extract_batch_features(
        self,
        images: List[Union[str, Image.Image]],
        batch_size: int = 32,
        normalize: bool = True
    ) -> np.ndarray:
        """
        æ‰¹é‡æå–å›¾ç‰‡ç‰¹å¾ï¼ˆæé«˜æ•ˆç‡ï¼‰
        
        Args:
            images: å›¾ç‰‡è·¯å¾„æˆ– PIL Image å¯¹è±¡åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°
            normalize: æ˜¯å¦å½’ä¸€åŒ–
            
        Returns:
            ç‰¹å¾çŸ©é˜µ (N x D)
        """
        all_features = []
        
        for i in range(0, len(images), batch_size):
            batch = images[i:i + batch_size]
            
            # åŠ è½½å›¾ç‰‡
            pil_images = []
            for img in batch:
                if isinstance(img, str):
                    pil_images.append(Image.open(img).convert("RGB"))
                else:
                    pil_images.append(img)
            
            # é¢„å¤„ç†
            inputs = self.processor(
                images=pil_images,
                return_tensors="pt",
                padding=True
            ).to(self.device)
            
            # æå–ç‰¹å¾
            with torch.no_grad():
                image_features = self.model.get_image_features(**inputs)
            
            # è½¬æ¢ä¸º numpy
            features = image_features.cpu().numpy()
            
            # å½’ä¸€åŒ–
            if normalize:
                features = features / np.linalg.norm(features, axis=1, keepdims=True)
            
            all_features.append(features)
        
        return np.vstack(all_features).astype(np.float32)
    
    def get_embedding_dim(self) -> int:
        """è·å–ç‰¹å¾ç»´åº¦"""
        return self.embedding_dim
    
    def get_model_info(self) -> dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "model_name": self.model_name,
            "embedding_dim": self.embedding_dim,
            "device": self.device,
            "cache_dir": self.cache_dir
        }


# å…¨å±€å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
_embedder = None


def get_embedder() -> CLIPEmbedder:
    """è·å–å…¨å±€ embedder å®ä¾‹"""
    global _embedder
    if _embedder is None:
        _embedder = CLIPEmbedder()
    return _embedder

