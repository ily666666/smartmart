#!/usr/bin/env python3
"""
å‡†ç¡®ç‡è¯„ä¼°è„šæœ¬

ä» vision_samples è¡¨ä¸­è¯»å–å·²ç¡®è®¤çš„è¯†åˆ«è®°å½•ï¼Œè®¡ç®—å‡†ç¡®ç‡æŒ‡æ ‡

ä½¿ç”¨æ–¹æ³•ï¼š
    cd backend
    python scripts/evaluate.py --db ./smartmart.db
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import argparse
import json
import sqlite3
from pathlib import Path
from typing import List, Dict
from collections import defaultdict


def load_samples(db_path: str) -> List[Dict]:
    """ä»æ•°æ®åº“åŠ è½½å·²ç¡®è®¤çš„æ ·æœ¬"""
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT 
            id,
            image_path,
            device_id,
            device_type,
            model_version,
            top_k_results,
            confirmed_sku_id,
            confirmed_score,
            created_at
        FROM vision_samples
        WHERE confirmed_sku_id IS NOT NULL
        ORDER BY created_at DESC
    """)
    
    samples = []
    for row in cursor.fetchall():
        samples.append({
            "id": row[0],
            "image_path": row[1],
            "device_id": row[2],
            "device_type": row[3],
            "model_version": row[4],
            "top_k_results": json.loads(row[5]) if row[5] else [],
            "confirmed_sku_id": row[6],
            "confirmed_score": row[7],
            "created_at": row[8]
        })
    
    conn.close()
    return samples


def calculate_metrics(samples: List[Dict]) -> Dict:
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    if not samples:
        return None
    
    num_samples = len(samples)
    
    # Top-1 å‡†ç¡®ç‡
    top1_correct = 0
    
    # Top-K å‡†ç¡®ç‡
    topk_correct = 0
    
    # MRR (Mean Reciprocal Rank)
    mrr_sum = 0
    
    # åˆ†æ•°ç»Ÿè®¡
    confirmed_scores = []
    top1_scores = []
    
    # æŒ‰ SKU åˆ†ç»„ç»Ÿè®¡
    sku_stats = defaultdict(lambda: {"correct": 0, "total": 0})
    
    # æŒ‰è®¾å¤‡ç±»å‹åˆ†ç»„
    device_stats = defaultdict(lambda: {"correct": 0, "total": 0})
    
    for sample in samples:
        top_k = sample["top_k_results"]
        confirmed_sku = sample["confirmed_sku_id"]
        device_type = sample["device_type"]
        
        if not top_k:
            continue
        
        # Top-1
        if top_k[0]["sku_id"] == confirmed_sku:
            top1_correct += 1
            sku_stats[confirmed_sku]["correct"] += 1
            device_stats[device_type]["correct"] += 1
        
        sku_stats[confirmed_sku]["total"] += 1
        device_stats[device_type]["total"] += 1
        
        # Top-K
        sku_ids = [item["sku_id"] for item in top_k]
        if confirmed_sku in sku_ids:
            topk_correct += 1
            rank = sku_ids.index(confirmed_sku) + 1
            mrr_sum += 1.0 / rank
        
        # åˆ†æ•°ç»Ÿè®¡
        confirmed_scores.append(sample.get("confirmed_score", 0))
        top1_scores.append(top_k[0]["score"])
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = {
        "num_samples": num_samples,
        "top1_accuracy": top1_correct / num_samples,
        "topk_accuracy": topk_correct / num_samples,
        "mrr": mrr_sum / num_samples,
        "avg_confirmed_score": sum(confirmed_scores) / len(confirmed_scores) if confirmed_scores else 0,
        "avg_top1_score": sum(top1_scores) / len(top1_scores) if top1_scores else 0,
        "sku_stats": dict(sku_stats),
        "device_stats": dict(device_stats)
    }
    
    return metrics


def print_report(metrics: Dict):
    """æ‰“å°è¯„ä¼°æŠ¥å‘Š"""
    print("=" * 70)
    print("ğŸ“Š è¯†åˆ«å‡†ç¡®ç‡è¯„ä¼°æŠ¥å‘Š")
    print("=" * 70)
    
    if not metrics:
        print("âŒ æ— å¯è¯„ä¼°çš„æ ·æœ¬ï¼ˆéœ€è¦ç”¨æˆ·ç¡®è®¤è¯†åˆ«ç»“æœï¼‰")
        return
    
    print(f"\næ€»ä½“æŒ‡æ ‡ (æ ·æœ¬æ•°: {metrics['num_samples']})")
    print("-" * 70)
    print(f"  Top-1 å‡†ç¡®ç‡:  {metrics['top1_accuracy']:.2%}")
    print(f"  Top-5 å‡†ç¡®ç‡:  {metrics['topk_accuracy']:.2%}")
    print(f"  MRR:          {metrics['mrr']:.3f}")
    print(f"  å¹³å‡ç½®ä¿¡åº¦:    {metrics['avg_top1_score']:.2%}")
    
    # æŒ‰ SKU ç»Ÿè®¡
    print(f"\næŒ‰å•†å“ç»Ÿè®¡ (Top 10 æœ€å¤šè¯†åˆ«)")
    print("-" * 70)
    sku_stats = sorted(
        metrics["sku_stats"].items(),
        key=lambda x: x[1]["total"],
        reverse=True
    )[:10]
    
    for sku_id, stats in sku_stats:
        accuracy = stats["correct"] / stats["total"]
        print(f"  SKU {sku_id:3d}: {accuracy:5.1%} ({stats['correct']}/{stats['total']})")
    
    # æŒ‰è®¾å¤‡ç±»å‹ç»Ÿè®¡
    print(f"\næŒ‰è®¾å¤‡ç±»å‹ç»Ÿè®¡")
    print("-" * 70)
    for device_type, stats in metrics["device_stats"].items():
        accuracy = stats["correct"] / stats["total"]
        print(f"  {device_type:12s}: {accuracy:5.1%} ({stats['correct']}/{stats['total']})")
    
    # è´¨é‡è¯„ä¼°
    print(f"\nè´¨é‡è¯„ä¼°")
    print("-" * 70)
    top1_acc = metrics['top1_accuracy']
    
    if top1_acc >= 0.9:
        quality = "ğŸŸ¢ ä¼˜ç§€"
    elif top1_acc >= 0.75:
        quality = "ğŸŸ¡ è‰¯å¥½"
    elif top1_acc >= 0.6:
        quality = "ğŸŸ  ä¸€èˆ¬"
    else:
        quality = "ğŸ”´ éœ€æ”¹è¿›"
    
    print(f"  æ¨¡å‹è´¨é‡: {quality}")
    
    if top1_acc < 0.75:
        print(f"\næ”¹è¿›å»ºè®®:")
        print(f"  1. å¢åŠ æ¯ä¸ª SKU çš„æ ·æœ¬æ•°é‡ï¼ˆå»ºè®® 5-10 å¼ ï¼‰")
        print(f"  2. é‡‡é›†å¤šè§’åº¦ã€å¤šåœºæ™¯å›¾ç‰‡")
        print(f"  3. æ”¶é›†è¯†åˆ«é”™è¯¯çš„å›°éš¾æ ·æœ¬")
        print(f"  4. è€ƒè™‘ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ï¼ˆCLIP-ViT-L/14ï¼‰")
        print(f"  5. å®šæœŸé‡å»ºç´¢å¼•ä»¥ä¼˜åŒ–è´¨é‡")
    
    print("\n" + "=" * 70)


def analyze_errors(samples: List[Dict], top_n: int = 10):
    """åˆ†æè¯†åˆ«é”™è¯¯çš„æ¡ˆä¾‹"""
    errors = []
    
    for sample in samples:
        top_k = sample["top_k_results"]
        confirmed_sku = sample["confirmed_sku_id"]
        
        if not top_k:
            continue
        
        # æ‰¾å‡º Top-1 é”™è¯¯
        if top_k[0]["sku_id"] != confirmed_sku:
            errors.append({
                "sample_id": sample["id"],
                "image_path": sample["image_path"],
                "predicted_sku": top_k[0]["sku_id"],
                "predicted_score": top_k[0]["score"],
                "actual_sku": confirmed_sku,
                "rank": next(
                    (i + 1 for i, item in enumerate(top_k) if item["sku_id"] == confirmed_sku),
                    -1
                )
            })
    
    if not errors:
        print("\nğŸ‰ å¤ªæ£’äº†ï¼æ‰€æœ‰æ ·æœ¬ Top-1 éƒ½è¯†åˆ«æ­£ç¡®ï¼")
        return
    
    print(f"\nâŒ é”™è¯¯æ¡ˆä¾‹åˆ†æ (å…± {len(errors)} ä¸ªé”™è¯¯)")
    print("=" * 70)
    
    # æŒ‰é¢„æµ‹åˆ†æ•°é™åºæ’åºï¼ˆé«˜ç½®ä¿¡åº¦é”™è¯¯æ›´éœ€è¦å…³æ³¨ï¼‰
    errors.sort(key=lambda x: x["predicted_score"], reverse=True)
    
    for i, error in enumerate(errors[:top_n], 1):
        print(f"\né”™è¯¯ {i}:")
        print(f"  æ ·æœ¬ID:    {error['sample_id']}")
        print(f"  å›¾ç‰‡è·¯å¾„:  {error['image_path']}")
        print(f"  é¢„æµ‹ SKU:  {error['predicted_sku']} (ç½®ä¿¡åº¦ {error['predicted_score']:.1%})")
        print(f"  å®é™… SKU:  {error['actual_sku']}")
        if error['rank'] > 0:
            print(f"  å®é™…æ’å:  ç¬¬ {error['rank']} ä½")
        else:
            print(f"  å®é™…æ’å:  æœªåœ¨ Top-K ä¸­")
    
    print("\n" + "=" * 70)


def main():
    parser = argparse.ArgumentParser(description="è¯„ä¼°è¯†åˆ«å‡†ç¡®ç‡")
    parser.add_argument(
        "--db",
        type=str,
        default="./smartmart.db",
        help="æ•°æ®åº“æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--errors",
        action="store_true",
        help="æ˜¾ç¤ºé”™è¯¯æ¡ˆä¾‹åˆ†æ"
    )
    parser.add_argument(
        "--top_errors",
        type=int,
        default=10,
        help="æ˜¾ç¤ºå‰ N ä¸ªé”™è¯¯æ¡ˆä¾‹"
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶
    if not Path(args.db).exists():
        print(f"âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {args.db}")
        return
    
    # åŠ è½½æ ·æœ¬
    print(f"ğŸ“‚ åŠ è½½æ•°æ®: {args.db}")
    samples = load_samples(args.db)
    
    if not samples:
        print("âŒ æœªæ‰¾åˆ°å·²ç¡®è®¤çš„è¯†åˆ«æ ·æœ¬")
        print("\nğŸ’¡ æç¤º:")
        print("   1. ä½¿ç”¨å°ç¨‹åºæ‹ç…§è¯†åˆ«")
        print("   2. ä»å€™é€‰åˆ—è¡¨ä¸­é€‰æ‹©æ­£ç¡®å•†å“")
        print("   3. ç³»ç»Ÿä¼šè‡ªåŠ¨è®°å½•ç¡®è®¤ç»“æœ")
        return
    
    print(f"âœ… åŠ è½½ {len(samples)} ä¸ªå·²ç¡®è®¤æ ·æœ¬\n")
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = calculate_metrics(samples)
    
    # æ‰“å°æŠ¥å‘Š
    print_report(metrics)
    
    # é”™è¯¯åˆ†æ
    if args.errors:
        analyze_errors(samples, top_n=args.top_errors)


if __name__ == "__main__":
    main()

